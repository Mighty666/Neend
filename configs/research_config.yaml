# NeendAI Research Configuration
# Configuration for research-grade experiments

# Data Configuration
data:
  datasets:
    - name: "Sleep-EDF"
      url: "https://physionet.org/content/sleep-edfx/1.0.0/"
      citation: "Kemp2000"
    - name: "SHHS"
      url: "https://sleepdata.org/datasets/shhs"
      citation: "Quan1997"
    - name: "PhysioNet Apnea-ECG"
      url: "https://physionet.org/content/apnea-ecg/1.0.0/"
      citation: "Penzel2000"
    - name: "A3"
      url: "https://physionet.org/content/challenge-2018/"
      citation: "Ghassemi2018"

  sample_rates: [16000, 48000]
  segment_duration: 30.0  # seconds

  preprocessing:
    denoising_methods:
      - spectral_subtraction
      - wiener
      - rnnoise

    time_frequency:
      n_mels: [64, 128, 256]
      n_fft: 2048
      hop_length: 512
      n_mfcc: 40
      wavelet_scales: 128

# Model Configuration
models:
  foundation:
    hidden_dim: 1024
    num_layers: 24
    num_heads: 16
    ff_dim: 4096
    dropout: 0.1
    patch_size: 16
    max_length: 2048
    mask_ratio: 0.75

  ssl_pretraining:
    model_types:
      - wav2vec2
      - hubert
      - byol_a
      - masked_spec

    training:
      max_steps: 400000
      warmup_steps: 32000
      batch_size: 32
      gradient_accumulation: 8
      learning_rate: 5e-4
      use_fp16: true

# Hyperparameter Search
hyperopt:
  n_trials: 2000
  timeout: 86400  # 24 hours
  sampler: tpe
  pruning: true

  search_spaces:
    cnn:
      n_filters: [32, 512]
      kernel_size: [3, 5, 7]
      dropout: [0.1, 0.5]
      learning_rate: [1e-5, 1e-2]

    transformer:
      hidden_dim: [256, 1024]
      num_layers: [4, 24]
      num_heads: [4, 16]
      learning_rate: [1e-5, 1e-3]

# Statistical Analysis
statistics:
  bootstrap_samples: 1000
  ci_level: 0.95
  significance_level: 0.05

  metrics:
    - auroc
    - auprc
    - sensitivity_at_specificity
    - brier_score
    - ece

# Experiment Tracking
tracking:
  use_mlflow: true
  use_wandb: false
  tracking_uri: "mlruns"
  experiment_name: "sleep_apnea_detection"

# Compute Resources
compute:
  distributed_backend: ray  # ray or dask
  n_gpus: 8
  n_cpus: 32
  memory_per_worker: "8GB"

  estimated_gpu_hours:
    ssl_pretraining: 800
    hyperopt_search: 400
    ablation_studies: 200
    foundation_training: 600

# Output Configuration
output:
  models_dir: "research/models"
  results_dir: "research/experiments"
  artifacts_dir: "research/artifacts"
  reports_dir: "research/reports"
