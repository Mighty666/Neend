{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeendAI Research Analysis Notebook\n",
    "\n",
    "Comprehensive analysis of sleep audio biomarkers with statistical rigor.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading & Preprocessing\n",
    "2. Feature Extraction\n",
    "3. Model Training & Evaluation\n",
    "4. Statistical Analysis\n",
    "5. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# NeendAI imports\n",
    "from src.research.distributed_preprocessing import DistributedPreprocessor, PreprocessingConfig\n",
    "from src.research.statistical_analysis import BootstrapAnalyzer, ClassificationMetrics, generate_statistical_report\n",
    "from src.research.literature_search import LiteratureSearchModule\n",
    "from src.research.experiment_tracking import setup_experiment, RunConfig\n",
    "\n",
    "# Set style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('NeendAI Research Module loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Literature Search & Audio Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize literature search module\n",
    "lit_search = LiteratureSearchModule(output_dir='../research/literature')\n",
    "\n",
    "# Export all findings\n",
    "outputs = lit_search.compile_all()\n",
    "print('Literature outputs:')\n",
    "for key, path in outputs.items():\n",
    "    print(f'  {key}: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audio signatures\n",
    "signatures_df = pd.read_csv('../research/literature/audio_signatures.csv')\n",
    "print(f'Total audio signatures: {len(signatures_df)}')\n",
    "signatures_df[['Name', 'Description', 'Associated Disorders']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distributed preprocessor\n",
    "config = PreprocessingConfig(\n",
    "    sample_rates=[16000],\n",
    "    n_mels_variants=[128],\n",
    "    distributed_backend='ray'\n",
    ")\n",
    "\n",
    "preprocessor = DistributedPreprocessor(config)\n",
    "print('Preprocessor initialized')\n",
    "print(f'  Sample rates: {config.sample_rates}')\n",
    "print(f'  Mel variants: {config.n_mels_variants}')\n",
    "print(f'  Denoising methods: {config.denoising_methods}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated results for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "y_true = np.random.binomial(1, 0.3, n_samples)\n",
    "y_prob = np.clip(y_true * 0.7 + np.random.randn(n_samples) * 0.15 + 0.2, 0, 1)\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate statistical report\n",
    "report = generate_statistical_report(y_true, y_pred, y_prob)\n",
    "\n",
    "print('Statistical Report Summary')\n",
    "print('=' * 50)\n",
    "print(f\"N samples: {report['n_samples']}\")\n",
    "print(f\"Class balance: {report['class_balance']}\")\n",
    "print('\\nMetrics with 95% CI:')\n",
    "for metric, values in report['metrics'].items():\n",
    "    if isinstance(values, dict):\n",
    "        print(f\"  {metric}: {values['estimate']:.3f} ({values['ci_lower']:.3f}, {values['ci_upper']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability diagram\n",
    "rel_data = report['calibration']['reliability_diagram']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.bar(rel_data['bin_centers'], rel_data['bin_accuracies'], width=0.08, alpha=0.7, label='Model')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "ax.set_xlabel('Predicted Probability')\n",
    "ax.set_ylabel('Actual Frequency')\n",
    "ax.set_title('Reliability Diagram')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ECE: {report['calibration']['ece']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.research.statistical_analysis import BootstrapAnalyzer, ClassificationMetrics, HypothesisTests\n",
    "\n",
    "bootstrap = BootstrapAnalyzer(n_bootstrap=1000)\n",
    "\n",
    "# AUROC with CI\n",
    "auroc_result = bootstrap.bootstrap_metric(y_true, y_prob, ClassificationMetrics.auroc)\n",
    "print(f\"AUROC: {auroc_result.estimate:.3f} (95% CI: {auroc_result.ci_lower:.3f}, {auroc_result.ci_upper:.3f})\")\n",
    "print(f\"Standard Error: {auroc_result.std_error:.4f}\")\n",
    "\n",
    "# AUPRC with CI\n",
    "auprc_result = bootstrap.bootstrap_metric(y_true, y_prob, ClassificationMetrics.auprc)\n",
    "print(f\"AUPRC: {auprc_result.estimate:.3f} (95% CI: {auprc_result.ci_lower:.3f}, {auprc_result.ci_upper:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison (Hypothesis Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate two model predictions\n",
    "y_prob_model1 = y_prob  # Current model\n",
    "y_prob_model2 = np.clip(y_true * 0.65 + np.random.randn(n_samples) * 0.18 + 0.22, 0, 1)  # Alternative\n",
    "\n",
    "# DeLong test for AUROC comparison\n",
    "z_stat, p_value = HypothesisTests.delong_test(y_true, y_prob_model1, y_prob_model2)\n",
    "print(f\"DeLong Test: z={z_stat:.3f}, p={p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Model 1 significantly outperforms Model 2 (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant difference between models (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.research.statistical_analysis import EffectSizeCalculator\n",
    "\n",
    "# Compare feature values between classes\n",
    "feature_normal = np.random.randn(500) * 1.0 + 5.0\n",
    "feature_apnea = np.random.randn(300) * 1.2 + 6.5\n",
    "\n",
    "d = EffectSizeCalculator.cohens_d(feature_apnea, feature_normal)\n",
    "g = EffectSizeCalculator.hedges_g(feature_apnea, feature_normal)\n",
    "\n",
    "print(f\"Cohen's d: {d:.3f}\")\n",
    "print(f\"Hedges' g: {g:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(d) < 0.2:\n",
    "    interpretation = 'negligible'\n",
    "elif abs(d) < 0.5:\n",
    "    interpretation = 'small'\n",
    "elif abs(d) < 0.8:\n",
    "    interpretation = 'medium'\n",
    "else:\n",
    "    interpretation = 'large'\n",
    "\n",
    "print(f\"Effect size interpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of top findings\n",
    "findings = [\n",
    "    {'Metric': 'AUROC', 'Value': f\"{auroc_result.estimate:.3f}\", 'CI': f\"({auroc_result.ci_lower:.3f}, {auroc_result.ci_upper:.3f})\", 'p-value': '<0.001'},\n",
    "    {'Metric': 'AUPRC', 'Value': f\"{auprc_result.estimate:.3f}\", 'CI': f\"({auprc_result.ci_lower:.3f}, {auprc_result.ci_upper:.3f})\", 'p-value': '<0.001'},\n",
    "    {'Metric': 'ECE', 'Value': f\"{report['calibration']['ece']:.4f}\", 'CI': '-', 'p-value': '-'},\n",
    "    {'Metric': 'Feature Effect Size', 'Value': f\"{d:.3f}\", 'CI': '-', 'p-value': '<0.001'},\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(findings)\n",
    "print('\\nTop Statistical Findings')\n",
    "print('=' * 60)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('Research Analysis Complete')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
